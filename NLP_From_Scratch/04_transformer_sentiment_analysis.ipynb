{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/envs/vision/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.optim import SGD,lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device=('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=datasets.load_dataset('imdb',\n",
    "                                           split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to tokenize it \n",
    "tokenizer=torchtext.data.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'heard',\n",
       " 'that',\n",
       " 'at',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'seized',\n",
       " 'by',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'customs',\n",
       " 'if',\n",
       " 'it',\n",
       " 'ever',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'this',\n",
       " 'country',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'being',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'films',\n",
       " 'considered',\n",
       " 'controversial',\n",
       " 'i',\n",
       " 'really',\n",
       " 'had',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'for',\n",
       " 'myself',\n",
       " '.',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'is',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'young',\n",
       " 'swedish',\n",
       " 'drama',\n",
       " 'student',\n",
       " 'named',\n",
       " 'lena',\n",
       " 'who',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'she',\n",
       " 'can',\n",
       " 'about',\n",
       " 'life',\n",
       " '.',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'she',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'focus',\n",
       " 'her',\n",
       " 'attentions',\n",
       " 'to',\n",
       " 'making',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'documentary',\n",
       " 'on',\n",
       " 'what',\n",
       " 'the',\n",
       " 'average',\n",
       " 'swede',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'certain',\n",
       " 'political',\n",
       " 'issues',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'vietnam',\n",
       " 'war',\n",
       " 'and',\n",
       " 'race',\n",
       " 'issues',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'in',\n",
       " 'between',\n",
       " 'asking',\n",
       " 'politicians',\n",
       " 'and',\n",
       " 'ordinary',\n",
       " 'denizens',\n",
       " 'of',\n",
       " 'stockholm',\n",
       " 'about',\n",
       " 'their',\n",
       " 'opinions',\n",
       " 'on',\n",
       " 'politics',\n",
       " ',',\n",
       " 'she',\n",
       " 'has',\n",
       " 'sex',\n",
       " 'with',\n",
       " 'her',\n",
       " 'drama',\n",
       " 'teacher',\n",
       " ',',\n",
       " 'classmates',\n",
       " ',',\n",
       " 'and',\n",
       " 'married',\n",
       " 'men',\n",
       " '.',\n",
       " 'what',\n",
       " 'kills',\n",
       " 'me',\n",
       " 'about',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'is',\n",
       " 'that',\n",
       " '40',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'this',\n",
       " 'was',\n",
       " 'considered',\n",
       " 'pornographic',\n",
       " '.',\n",
       " 'really',\n",
       " ',',\n",
       " 'the',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nudity',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'few',\n",
       " 'and',\n",
       " 'far',\n",
       " 'between',\n",
       " ',',\n",
       " 'even',\n",
       " 'then',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'shot',\n",
       " 'like',\n",
       " 'some',\n",
       " 'cheaply',\n",
       " 'made',\n",
       " 'porno',\n",
       " '.',\n",
       " 'while',\n",
       " 'my',\n",
       " 'countrymen',\n",
       " 'mind',\n",
       " 'find',\n",
       " 'it',\n",
       " 'shocking',\n",
       " ',',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nudity',\n",
       " 'are',\n",
       " 'a',\n",
       " 'major',\n",
       " 'staple',\n",
       " 'in',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'even',\n",
       " 'ingmar',\n",
       " 'bergman',\n",
       " ',',\n",
       " 'arguably',\n",
       " 'their',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'good',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'john',\n",
       " 'ford',\n",
       " ',',\n",
       " 'had',\n",
       " 'sex',\n",
       " 'scenes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'films',\n",
       " '.',\n",
       " 'i',\n",
       " 'do',\n",
       " 'commend',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'any',\n",
       " 'sex',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " 'is',\n",
       " 'shown',\n",
       " 'for',\n",
       " 'artistic',\n",
       " 'purposes',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'just',\n",
       " 'to',\n",
       " 'shock',\n",
       " 'people',\n",
       " 'and',\n",
       " 'make',\n",
       " 'money',\n",
       " 'to',\n",
       " 'be',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'pornographic',\n",
       " 'theaters',\n",
       " 'in',\n",
       " 'america',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious-yellow',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'film',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'wanting',\n",
       " 'to',\n",
       " 'study',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'potatoes',\n",
       " '(',\n",
       " 'no',\n",
       " 'pun',\n",
       " 'intended',\n",
       " ')',\n",
       " 'of',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'but',\n",
       " 'really',\n",
       " ',',\n",
       " 'this',\n",
       " 'film',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'plot',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "tokenizer(train_data[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(example_data,tokenizer,max_length):\n",
    "    tokens=tokenizer(example_data['text'])[:max_length]\n",
    "    return {'tokens':tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##applying map funtion\n",
    "max_length=256\n",
    "train_data=train_data.map(tokenize_text,\n",
    "                          fn_kwargs={'tokenizer':tokenizer,\n",
    "                                      'max_length':max_length})\n",
    "test_data=test_data.map(tokenize_text,\n",
    "                          fn_kwargs={'tokenizer':tokenizer,\n",
    "                                      'max_length':max_length})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]['tokens']) ##len will be <=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now apply vocabulary\n",
    "min_frequency=5\n",
    "specials=['unk','pad']\n",
    "\n",
    "vocab=torchtext.vocab.build_vocab_from_iterator(train_data['tokens'],\n",
    "                                                min_freq=min_frequency,\n",
    "                                                specials=specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 1242,\n",
       " 12,\n",
       " 220,\n",
       " 0,\n",
       " 44,\n",
       " 61,\n",
       " 362,\n",
       " 1035,\n",
       " 90,\n",
       " 7,\n",
       " 37,\n",
       " 2,\n",
       " 7142,\n",
       " 15,\n",
       " 3319,\n",
       " 11,\n",
       " 60,\n",
       " 11,\n",
       " 17,\n",
       " 80,\n",
       " 569,\n",
       " 13,\n",
       " 7558,\n",
       " 3,\n",
       " 12,\n",
       " 99,\n",
       " 508,\n",
       " 15,\n",
       " 38,\n",
       " 80,\n",
       " 11,\n",
       " 17,\n",
       " 24371,\n",
       " 40,\n",
       " 1095,\n",
       " 3,\n",
       " 16,\n",
       " 3,\n",
       " 10339,\n",
       " 52,\n",
       " 11,\n",
       " 125,\n",
       " 747,\n",
       " 8,\n",
       " 2389,\n",
       " 14,\n",
       " 644,\n",
       " 4,\n",
       " 1644,\n",
       " 123,\n",
       " 5,\n",
       " 314,\n",
       " 7,\n",
       " 116,\n",
       " 1121,\n",
       " 3029,\n",
       " 12,\n",
       " 68,\n",
       " 72,\n",
       " 8,\n",
       " 73,\n",
       " 14,\n",
       " 21,\n",
       " 496,\n",
       " 3,\n",
       " 2,\n",
       " 114,\n",
       " 10,\n",
       " 5778,\n",
       " 195,\n",
       " 5,\n",
       " 182,\n",
       " 3517,\n",
       " 442,\n",
       " 1306,\n",
       " 726,\n",
       " 5178,\n",
       " 42,\n",
       " 509,\n",
       " 8,\n",
       " 865,\n",
       " 293,\n",
       " 63,\n",
       " 59,\n",
       " 47,\n",
       " 126,\n",
       " 3,\n",
       " 13,\n",
       " 859,\n",
       " 63,\n",
       " 509,\n",
       " 8,\n",
       " 1157,\n",
       " 51,\n",
       " 11837,\n",
       " 8,\n",
       " 263,\n",
       " 55,\n",
       " 457,\n",
       " 7,\n",
       " 606,\n",
       " 27,\n",
       " 54,\n",
       " 2,\n",
       " 811,\n",
       " 0,\n",
       " 190,\n",
       " 47,\n",
       " 805,\n",
       " 1045,\n",
       " 1284,\n",
       " 145,\n",
       " 19,\n",
       " 2,\n",
       " 2353,\n",
       " 331,\n",
       " 6,\n",
       " 1506,\n",
       " 1284,\n",
       " 13,\n",
       " 2,\n",
       " 2238,\n",
       " 1530,\n",
       " 3,\n",
       " 13,\n",
       " 215,\n",
       " 2240,\n",
       " 6937,\n",
       " 6,\n",
       " 1940,\n",
       " 16876,\n",
       " 7,\n",
       " 18792,\n",
       " 47,\n",
       " 77,\n",
       " 4485,\n",
       " 27,\n",
       " 2307,\n",
       " 4,\n",
       " 63,\n",
       " 50,\n",
       " 405,\n",
       " 20,\n",
       " 51,\n",
       " 442,\n",
       " 1537,\n",
       " 4,\n",
       " 6999,\n",
       " 4,\n",
       " 6,\n",
       " 979,\n",
       " 366,\n",
       " 3,\n",
       " 54,\n",
       " 1150,\n",
       " 75,\n",
       " 47,\n",
       " 12,\n",
       " 220,\n",
       " 0,\n",
       " 10,\n",
       " 15,\n",
       " 1577,\n",
       " 152,\n",
       " 529,\n",
       " 4,\n",
       " 14,\n",
       " 17,\n",
       " 1121,\n",
       " 9575,\n",
       " 3,\n",
       " 68,\n",
       " 4,\n",
       " 2,\n",
       " 405,\n",
       " 6,\n",
       " 973,\n",
       " 150,\n",
       " 30,\n",
       " 175,\n",
       " 6,\n",
       " 244,\n",
       " 215,\n",
       " 4,\n",
       " 67,\n",
       " 103,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 29,\n",
       " 351,\n",
       " 45,\n",
       " 55,\n",
       " 5867,\n",
       " 97,\n",
       " 4098,\n",
       " 3,\n",
       " 147,\n",
       " 61,\n",
       " 22992,\n",
       " 357,\n",
       " 171,\n",
       " 11,\n",
       " 1636,\n",
       " 4,\n",
       " 13,\n",
       " 664,\n",
       " 405,\n",
       " 6,\n",
       " 973,\n",
       " 30,\n",
       " 5,\n",
       " 648,\n",
       " 9632,\n",
       " 13,\n",
       " 3517,\n",
       " 437,\n",
       " 3,\n",
       " 67,\n",
       " 14399,\n",
       " 4202,\n",
       " 4,\n",
       " 4150,\n",
       " 77,\n",
       " 1583,\n",
       " 8,\n",
       " 57,\n",
       " 165,\n",
       " 407,\n",
       " 291,\n",
       " 1933,\n",
       " 4,\n",
       " 72,\n",
       " 405,\n",
       " 150,\n",
       " 13,\n",
       " 32,\n",
       " 116,\n",
       " 3,\n",
       " 12,\n",
       " 93,\n",
       " 14970,\n",
       " 2,\n",
       " 1078,\n",
       " 21,\n",
       " 2,\n",
       " 202,\n",
       " 15,\n",
       " 110,\n",
       " 405,\n",
       " 628,\n",
       " 13,\n",
       " 2,\n",
       " 23,\n",
       " 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices(train_data[0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a function to vocabularize \n",
    "\n",
    "def vocabularize_tokens(example_data,vocab):\n",
    "    ids=vocab.lookup_indices(example_data['tokens'])\n",
    "    return {'ids':ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mapping to train_data and test_data\n",
    "\n",
    "train_data=train_data.map(vocabularize_tokens,\n",
    "                          fn_kwargs={'vocab':vocab})\n",
    "\n",
    "\n",
    "test_data=test_data.map(vocabularize_tokens,\n",
    "                          fn_kwargs={'vocab':vocab})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 1242,\n",
       " 12,\n",
       " 220,\n",
       " 0,\n",
       " 44,\n",
       " 61,\n",
       " 362,\n",
       " 1035,\n",
       " 90,\n",
       " 7,\n",
       " 37,\n",
       " 2,\n",
       " 7142,\n",
       " 15,\n",
       " 3319,\n",
       " 11,\n",
       " 60,\n",
       " 11,\n",
       " 17,\n",
       " 80,\n",
       " 569,\n",
       " 13,\n",
       " 7558,\n",
       " 3,\n",
       " 12,\n",
       " 99,\n",
       " 508,\n",
       " 15,\n",
       " 38,\n",
       " 80,\n",
       " 11,\n",
       " 17,\n",
       " 24371,\n",
       " 40,\n",
       " 1095,\n",
       " 3,\n",
       " 16,\n",
       " 3,\n",
       " 10339,\n",
       " 52,\n",
       " 11,\n",
       " 125,\n",
       " 747,\n",
       " 8,\n",
       " 2389,\n",
       " 14,\n",
       " 644,\n",
       " 4,\n",
       " 1644,\n",
       " 123,\n",
       " 5,\n",
       " 314,\n",
       " 7,\n",
       " 116,\n",
       " 1121,\n",
       " 3029,\n",
       " 12,\n",
       " 68,\n",
       " 72,\n",
       " 8,\n",
       " 73,\n",
       " 14,\n",
       " 21,\n",
       " 496,\n",
       " 3,\n",
       " 2,\n",
       " 114,\n",
       " 10,\n",
       " 5778,\n",
       " 195,\n",
       " 5,\n",
       " 182,\n",
       " 3517,\n",
       " 442,\n",
       " 1306,\n",
       " 726,\n",
       " 5178,\n",
       " 42,\n",
       " 509,\n",
       " 8,\n",
       " 865,\n",
       " 293,\n",
       " 63,\n",
       " 59,\n",
       " 47,\n",
       " 126,\n",
       " 3,\n",
       " 13,\n",
       " 859,\n",
       " 63,\n",
       " 509,\n",
       " 8,\n",
       " 1157,\n",
       " 51,\n",
       " 11837,\n",
       " 8,\n",
       " 263,\n",
       " 55,\n",
       " 457,\n",
       " 7,\n",
       " 606,\n",
       " 27,\n",
       " 54,\n",
       " 2,\n",
       " 811,\n",
       " 0,\n",
       " 190,\n",
       " 47,\n",
       " 805,\n",
       " 1045,\n",
       " 1284,\n",
       " 145,\n",
       " 19,\n",
       " 2,\n",
       " 2353,\n",
       " 331,\n",
       " 6,\n",
       " 1506,\n",
       " 1284,\n",
       " 13,\n",
       " 2,\n",
       " 2238,\n",
       " 1530,\n",
       " 3,\n",
       " 13,\n",
       " 215,\n",
       " 2240,\n",
       " 6937,\n",
       " 6,\n",
       " 1940,\n",
       " 16876,\n",
       " 7,\n",
       " 18792,\n",
       " 47,\n",
       " 77,\n",
       " 4485,\n",
       " 27,\n",
       " 2307,\n",
       " 4,\n",
       " 63,\n",
       " 50,\n",
       " 405,\n",
       " 20,\n",
       " 51,\n",
       " 442,\n",
       " 1537,\n",
       " 4,\n",
       " 6999,\n",
       " 4,\n",
       " 6,\n",
       " 979,\n",
       " 366,\n",
       " 3,\n",
       " 54,\n",
       " 1150,\n",
       " 75,\n",
       " 47,\n",
       " 12,\n",
       " 220,\n",
       " 0,\n",
       " 10,\n",
       " 15,\n",
       " 1577,\n",
       " 152,\n",
       " 529,\n",
       " 4,\n",
       " 14,\n",
       " 17,\n",
       " 1121,\n",
       " 9575,\n",
       " 3,\n",
       " 68,\n",
       " 4,\n",
       " 2,\n",
       " 405,\n",
       " 6,\n",
       " 973,\n",
       " 150,\n",
       " 30,\n",
       " 175,\n",
       " 6,\n",
       " 244,\n",
       " 215,\n",
       " 4,\n",
       " 67,\n",
       " 103,\n",
       " 11,\n",
       " 9,\n",
       " 16,\n",
       " 29,\n",
       " 351,\n",
       " 45,\n",
       " 55,\n",
       " 5867,\n",
       " 97,\n",
       " 4098,\n",
       " 3,\n",
       " 147,\n",
       " 61,\n",
       " 22992,\n",
       " 357,\n",
       " 171,\n",
       " 11,\n",
       " 1636,\n",
       " 4,\n",
       " 13,\n",
       " 664,\n",
       " 405,\n",
       " 6,\n",
       " 973,\n",
       " 30,\n",
       " 5,\n",
       " 648,\n",
       " 9632,\n",
       " 13,\n",
       " 3517,\n",
       " 437,\n",
       " 3,\n",
       " 67,\n",
       " 14399,\n",
       " 4202,\n",
       " 4,\n",
       " 4150,\n",
       " 77,\n",
       " 1583,\n",
       " 8,\n",
       " 57,\n",
       " 165,\n",
       " 407,\n",
       " 291,\n",
       " 1933,\n",
       " 4,\n",
       " 72,\n",
       " 405,\n",
       " 150,\n",
       " 13,\n",
       " 32,\n",
       " 116,\n",
       " 3,\n",
       " 12,\n",
       " 93,\n",
       " 14970,\n",
       " 2,\n",
       " 1078,\n",
       " 21,\n",
       " 2,\n",
       " 202,\n",
       " 15,\n",
       " 110,\n",
       " 405,\n",
       " 628,\n",
       " 13,\n",
       " 2,\n",
       " 23,\n",
       " 10]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['neg', 'pos'], id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating tensors data\n",
    "\n",
    "train_data=train_data.with_format(type='torch',columns=['ids','label'])\n",
    "test_data=test_data.with_format(type='torch',column=['ids','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0]['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##splitting train_data to train_val_data\n",
    "\n",
    "train_val_data=train_data.train_test_split(0.25)\n",
    "train_data=train_val_data['train']\n",
    "val_data=train_val_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14062"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fnc(pad_idx):\n",
    "\n",
    "    def collate_fnc(batch):\n",
    "        batch_ids=[i['ids'] for i in batch]\n",
    "        batch_label=[i['label'] for i in batch]\n",
    "        batch_ids=torch.nn.utils.rnn.pad_sequence(batch_ids,padding_value=pad_idx,batch_first=True)\n",
    "        batch={'ids':batch_ids,'label':batch_label}\n",
    "        return batch\n",
    "    \n",
    "    return collate_fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(dataset,batch_size,pad_idx,shuffle=False):\n",
    "    collate_fnc=get_collate_fnc(pad_idx)\n",
    "    dataloader=DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          collate_fn=collate_fnc,\n",
    "                          shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataloader(train_data,10,0,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 247])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 240])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 252])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 253])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 249])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 237])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 245])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 241])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 252])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 214])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 233])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 238])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 248])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 232])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###creating a mode model for sentiment analysis; however unlike in previous turtorial we will build TransformerEncoder from scratch, and use it inplace of LSTM\n",
    "\n",
    "### Defining Embedding Class now \n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.emb_layer=nn.Embedding(num_embeddings=self.vocab_size,embedding_dim=self.embed_dim)\n",
    "\n",
    "    def positional_encoding(self,seq_len, embed_dim):\n",
    "        pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        return pe\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.emb_layer(x)\n",
    "        seq_len=x.shape[1]\n",
    "        pos_enc=self.positional_encoding(seq_len=seq_len,embed_dim=self.embed_dim).to(device=x.device)\n",
    "        return x+pos_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFL(nn.Module): ## feedforward Layer\n",
    "    def __init__(self,embed_dim,dff,dropout):\n",
    "        super(FFL,self).__init__()\n",
    "        self.ffl_layer=nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim,out_features=dff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=dff,out_features=embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.ffl_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,vocab_size,droput,num_heads):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        ##initializing variables\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.dff=self.embed_dim*4\n",
    "        self.num_heads=num_heads\n",
    "        self.dropout=droput\n",
    "        \n",
    "        ##nn.NN\n",
    "        self.mha=nn.MultiheadAttention(embed_dim=self.embed_dim,num_heads=self.num_heads,dropout=self.dropout)\n",
    "        self.layer_nomr1=nn.LayerNorm(normalized_shape=self.embed_dim)\n",
    "        self.layer_nomr2=nn.LayerNorm(normalized_shape=self.embed_dim)\n",
    "\n",
    "        ##Class Objects\n",
    "        \n",
    "        self.feedforward_layer=FFL(embed_dim=self.embed_dim,dff=self.dff,dropout=self.dropout)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x is of shape batch_size,seq_len,embed_dim; output from Embedding Layer\n",
    "\n",
    "        \n",
    "        mha_output,_=self.mha(x,x,x)\n",
    "        \n",
    "        normlized_1=self.layer_nomr1(x+mha_output)\n",
    "        ffd_output=self.feedforward_layer(normlized_1)\n",
    "        normlized_2=self.layer_nomr2(normlized_1+ffd_output)\n",
    "\n",
    "        return normlized_2\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads=8\n",
    "num_layers=6\n",
    "### Creating Embedding Layer \n",
    "embed_dim=256\n",
    "vocab_size=100\n",
    "batch_size=8\n",
    "num_tokens=10\n",
    "encoder=TransformerEncoder(embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=0.3)\n",
    "\n",
    "encoder=encoder.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ids=batch['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining Encoder with num_of_layers\n",
    "class TransformerEncoderLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_layers,embed_dim,vocab_size,droput,num_heads):\n",
    "        super(TransformerEncoderLayers,self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.embedding_layers=EmbeddingLayer(vocab_size=vocab_size,embed_dim=embed_dim)\n",
    "        self.transformer_encoders=nn.ModuleList([TransformerEncoder(embed_dim,vocab_size,droput,num_heads)\n",
    "                                                for i in range(self.num_layers)])\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.embedding_layers(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x=self.transformer_encoders[i](x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking TransformerEncoder\n",
    "## defining Encoder Parameter using baseline from Attention is all we need Paper\n",
    "num_heads=8\n",
    "num_layers=6\n",
    "### Creating Embedding Layer \n",
    "embed_dim=256\n",
    "vocab_size=100\n",
    "batch_size=8\n",
    "num_tokens=10\n",
    "\n",
    "example_tensors=torch.rand(size=(batch_size,num_tokens)).to(dtype=torch.int64)\n",
    "encoder_layers=TransformerEncoderLayers(num_layers=num_layers,embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6860,  0.0186, -0.5737,  ..., -1.3529, -1.2014,  1.4205],\n",
       "         [-1.3166, -0.3840, -0.5542,  ..., -1.0626, -0.3838,  0.0923],\n",
       "         [-1.3858, -0.9728, -0.6733,  ..., -0.7308, -0.3352,  0.6164],\n",
       "         ...,\n",
       "         [-1.4923,  0.5200, -0.9967,  ..., -1.8046, -1.0522,  0.1120],\n",
       "         [-0.7978, -0.9199, -0.1573,  ..., -0.7895, -0.6820, -0.4502],\n",
       "         [-1.5262, -1.2595, -0.5342,  ..., -0.9933, -0.8044,  0.8380]],\n",
       "\n",
       "        [[-1.4762, -0.8589, -0.6190,  ..., -1.3217, -1.2633,  0.8759],\n",
       "         [-1.5596, -0.6869,  0.0458,  ..., -1.0617, -1.4319,  0.5335],\n",
       "         [-0.8491, -0.7903, -0.2657,  ..., -0.4611, -0.2816,  0.5894],\n",
       "         ...,\n",
       "         [-1.3151, -0.5618, -0.8633,  ..., -1.2598, -0.3456,  0.0689],\n",
       "         [-1.0664, -0.6998, -0.1316,  ..., -1.4276, -0.5668, -0.1251],\n",
       "         [-1.5471, -1.0636, -0.4235,  ..., -0.1274, -0.6342,  0.5107]],\n",
       "\n",
       "        [[-1.2409, -0.2825, -0.8003,  ..., -1.4103, -1.0526,  0.8120],\n",
       "         [-1.3440, -0.5117, -0.7935,  ..., -1.3850, -0.9788,  0.6847],\n",
       "         [-1.2636, -1.0569, -0.6363,  ..., -0.8040, -0.4635,  0.0613],\n",
       "         ...,\n",
       "         [-0.9242,  0.1909, -0.0705,  ..., -1.0832, -0.5301, -0.1754],\n",
       "         [-0.7963, -0.2464, -0.2716,  ..., -0.7658, -1.2049,  0.1290],\n",
       "         [-1.0367, -0.5947, -1.1102,  ..., -0.9389, -0.7237,  0.3239]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4292, -0.5686, -1.2366,  ..., -1.4326, -1.3540,  0.9557],\n",
       "         [-1.0130, -0.6773, -0.7358,  ..., -1.2220, -0.7942,  0.7688],\n",
       "         [-0.6917, -0.9774, -0.1305,  ..., -1.0999, -0.1617,  0.5273],\n",
       "         ...,\n",
       "         [-1.1182,  0.0339, -1.2381,  ..., -1.3709, -0.6281, -0.1252],\n",
       "         [-0.5918,  0.0020, -0.4759,  ..., -1.2036, -1.1775, -0.4040],\n",
       "         [-1.2575, -1.1464, -0.9923,  ..., -0.6759, -0.5993, -0.0759]],\n",
       "\n",
       "        [[-1.3639, -0.7405, -0.6376,  ..., -1.2005, -1.3927,  0.7333],\n",
       "         [-1.3626, -0.1617, -0.7366,  ..., -1.1586, -1.0606,  0.6106],\n",
       "         [-1.5326, -0.5489, -0.3420,  ..., -0.6261, -0.4540,  0.8420],\n",
       "         ...,\n",
       "         [-1.7240,  0.1529, -0.4709,  ..., -1.0357, -0.7086,  0.2699],\n",
       "         [ 0.0437, -0.3150, -0.6370,  ..., -0.7240, -1.0716,  0.1948],\n",
       "         [-1.1635, -1.1113, -0.4357,  ..., -0.8759, -1.0251, -0.1373]],\n",
       "\n",
       "        [[-1.1705,  0.1559, -0.6505,  ..., -1.4898, -1.2570,  1.6993],\n",
       "         [-0.9618, -0.8842, -0.5712,  ..., -0.9966, -0.4929,  0.8475],\n",
       "         [-1.5071, -1.1493,  0.0220,  ..., -1.0906, -0.8328,  0.7641],\n",
       "         ...,\n",
       "         [-1.1812,  0.0120, -0.1033,  ..., -1.3609, -0.5432,  0.0660],\n",
       "         [-0.7882, -0.8813, -0.4076,  ..., -1.4146, -0.1913,  0.1789],\n",
       "         [-1.0797, -0.6661, -0.8261,  ..., -0.5009, -0.9648,  0.3633]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating a random tensor  || Testing Custom Transformers encoder layers\n",
    "batch_size=8\n",
    "num_tokens=10\n",
    "\n",
    "example_tensors=torch.rand(size=(batch_size,num_tokens)).to(dtype=torch.int64)\n",
    "\n",
    "encoder_layers(example_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4859e+00, -6.2361e-01, -6.1226e-01,  ..., -1.0399e+00,\n",
       "          -1.4903e+00,  9.6051e-01],\n",
       "         [-9.5148e-01, -4.6248e-01, -3.1253e-01,  ..., -8.7676e-01,\n",
       "          -5.2341e-01,  5.0658e-01],\n",
       "         [-1.5375e+00, -1.0660e+00, -1.1916e+00,  ..., -9.9766e-01,\n",
       "          -1.0989e+00,  8.1634e-01],\n",
       "         ...,\n",
       "         [-6.2386e-01,  1.2346e-01, -5.1488e-01,  ..., -1.7369e+00,\n",
       "          -7.0508e-01,  2.1727e-01],\n",
       "         [-8.9607e-01, -9.3249e-01,  1.8323e-01,  ..., -1.3563e+00,\n",
       "          -1.1281e+00, -6.2004e-02],\n",
       "         [-1.2679e+00, -1.0861e+00, -6.4117e-01,  ..., -9.0980e-01,\n",
       "          -6.6566e-01, -5.1932e-02]],\n",
       "\n",
       "        [[-1.4801e+00, -4.6780e-01, -4.0603e-01,  ..., -1.8978e+00,\n",
       "          -4.7852e-01,  8.7885e-01],\n",
       "         [-1.5067e+00, -3.5352e-02, -2.0204e-01,  ..., -1.2274e+00,\n",
       "          -1.8952e-01,  6.3198e-01],\n",
       "         [-5.1526e-01, -7.0663e-01, -3.5956e-01,  ..., -9.1291e-01,\n",
       "          -1.0132e+00,  8.3923e-01],\n",
       "         ...,\n",
       "         [-1.0508e+00,  4.9232e-01, -1.0776e-01,  ..., -1.5215e+00,\n",
       "          -7.5548e-01,  1.2192e-01],\n",
       "         [-7.0760e-01, -9.5258e-01,  4.3521e-01,  ..., -1.2020e+00,\n",
       "          -2.5296e-01,  3.3391e-01],\n",
       "         [-1.1773e+00, -1.2012e+00, -4.5151e-01,  ..., -7.2727e-01,\n",
       "          -1.0219e+00,  8.8038e-02]],\n",
       "\n",
       "        [[-1.3761e+00, -4.8764e-02, -3.8464e-01,  ..., -1.3561e+00,\n",
       "          -9.6858e-01,  1.4194e+00],\n",
       "         [-4.5275e-01, -6.8632e-01,  5.0708e-02,  ..., -1.0561e+00,\n",
       "          -7.4911e-01,  6.1085e-01],\n",
       "         [-8.4402e-01, -1.3288e+00, -4.0605e-01,  ..., -9.1964e-01,\n",
       "          -7.8035e-01,  1.0152e+00],\n",
       "         ...,\n",
       "         [-1.7729e+00,  2.8678e-02, -5.8252e-01,  ..., -1.6981e+00,\n",
       "          -9.0100e-01,  1.8810e-01],\n",
       "         [-6.8843e-01,  4.7726e-02, -1.2137e+00,  ..., -1.5435e+00,\n",
       "          -8.5950e-01,  3.8172e-01],\n",
       "         [-1.5983e+00, -1.2734e+00, -8.9254e-01,  ..., -6.2138e-01,\n",
       "          -1.8052e-01, -4.6184e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.4055e+00,  1.4697e-01, -9.5553e-01,  ..., -1.4915e+00,\n",
       "          -1.1084e+00,  7.5741e-01],\n",
       "         [-1.0965e+00, -2.6204e-01, -1.6723e-01,  ..., -8.9908e-01,\n",
       "          -5.9634e-01,  5.7611e-01],\n",
       "         [-1.5441e+00, -1.3703e+00, -7.3142e-01,  ..., -4.7636e-01,\n",
       "          -5.3858e-01,  2.3128e-01],\n",
       "         ...,\n",
       "         [-1.4423e+00, -8.4301e-01, -3.6997e-01,  ..., -1.1631e+00,\n",
       "          -7.4192e-01,  1.3954e-01],\n",
       "         [-9.1885e-01, -7.0339e-01, -2.2676e-01,  ..., -1.3929e+00,\n",
       "          -1.0723e+00,  1.3509e-01],\n",
       "         [-1.3210e+00, -1.2748e+00, -4.4710e-01,  ..., -6.3706e-01,\n",
       "          -1.5894e-01, -9.6483e-02]],\n",
       "\n",
       "        [[-1.5518e+00, -8.3936e-01, -8.1345e-01,  ..., -1.6089e+00,\n",
       "          -1.0895e+00,  1.2583e+00],\n",
       "         [-8.9779e-01, -1.0059e+00, -3.7359e-01,  ..., -1.1586e+00,\n",
       "          -1.0311e+00,  4.9945e-01],\n",
       "         [-1.1865e+00, -7.8598e-01, -4.6873e-03,  ..., -6.6639e-01,\n",
       "          -3.2608e-01,  5.5974e-01],\n",
       "         ...,\n",
       "         [-1.1301e+00, -2.9526e-01, -1.1062e+00,  ..., -8.7179e-01,\n",
       "          -8.3812e-01,  8.9414e-01],\n",
       "         [-8.5833e-01, -3.8435e-01, -8.7626e-01,  ..., -1.1008e+00,\n",
       "          -6.6736e-01,  2.7311e-01],\n",
       "         [-1.3962e+00, -1.0642e+00, -7.7799e-02,  ..., -4.1038e-01,\n",
       "          -1.0279e+00,  9.5092e-02]],\n",
       "\n",
       "        [[-1.9343e+00, -3.7827e-01, -6.7984e-01,  ..., -1.1062e+00,\n",
       "          -9.9898e-01,  7.5823e-01],\n",
       "         [-8.4548e-01, -4.5588e-01, -3.4779e-01,  ..., -1.7677e+00,\n",
       "          -6.1789e-01,  8.2042e-01],\n",
       "         [-1.0543e+00, -1.0964e+00,  2.4861e-01,  ..., -6.8553e-01,\n",
       "          -9.0460e-01,  5.4143e-01],\n",
       "         ...,\n",
       "         [-1.0538e+00, -1.0714e-03, -4.4965e-01,  ..., -1.2165e+00,\n",
       "          -1.0618e+00, -1.2793e-01],\n",
       "         [-9.0304e-01,  1.0786e-05, -5.1417e-01,  ..., -1.3418e+00,\n",
       "          -1.4318e+00,  1.6045e-01],\n",
       "         [-1.3530e+00, -1.8297e+00, -4.3682e-01,  ..., -6.2482e-01,\n",
       "          -8.1750e-01,  3.9583e-01]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##tesitng in GPU cuda\n",
    "\n",
    "encoder_layers.to(device=device)(example_tensors.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## okay working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|| 14062/14062 [00:05<00:00, 2724.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### Now continue to build NBOW but first let us define the parameters\n",
    "vocab_size=len(vocab)\n",
    "embed_dim=512\n",
    "output_dim=len(train_data.unique('label'))\n",
    "\n",
    "## parameters for EncoderLayers\n",
    "num_heads=8\n",
    "num_layers=2\n",
    "dropout=0.3\n",
    "\n",
    "##\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim%num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def NBOW\n",
    "\n",
    "class NBOW_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,num_layers,embed_dim,vocab_size,num_heads,dropout,output_dim):\n",
    "        super(NBOW_Encoder,self).__init__()\n",
    "\n",
    "        self.transformer_encoder_layer=TransformerEncoderLayers(num_layers=num_layers,embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=dropout)\n",
    "        self.linear=nn.Linear(embed_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        encoder_output=self.transformer_encoder_layer(x)\n",
    "        flatten_output=encoder_output.mean(1)\n",
    "        final_output=self.linear(flatten_output)\n",
    "        return final_output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "##checking model\n",
    "ids=batch['ids'].to(device=device)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NBOW_Encoder(num_layers=num_layers,embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,dropout=dropout,\n",
    "                           output_dim=output_dim).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(ids),axis=1) ## working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "num_layers = 6  # Example value\n",
    "embed_dim = 512  # Example value\n",
    "vocab_size = 10000  # Example value\n",
    "dropout = 0.1  # Example value\n",
    "num_heads = 8  # Example value\n",
    "\n",
    "model = TransformerEncoderLayers(num_layers=num_layers, embed_dim=embed_dim, vocab_size=vocab_size, num_heads=num_heads,droput=dropout)\n",
    "\n",
    "# Move the entire model to the CUDA device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred_logits,true_label):\n",
    "    pred_label=torch.argmax(pred_logits,axis=1)\n",
    "    total_predictions=pred_label.eq(true_label).sum()\n",
    "    accuracy=total_predictions/pred_label.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "## testing \n",
    "pred_logits=torch.rand(size=(5,3))\n",
    "true_label=torch.tensor(data=[0,1,0,2,2])\n",
    "\n",
    "accuracy=get_accuracy(pred_logits,true_label)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataloader(dataset=train_data,batch_size=batch_size,pad_idx=0,shuffle=True)\n",
    "val_dataloader=dataloader(dataset=val_data,batch_size=batch_size,pad_idx=0,shuffle=False)\n",
    "test_dataloader=dataloader(dataset=test_data,batch_size=batch_size,pad_idx=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss().to(device=device)\n",
    "optimizer=SGD(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    for batch in tqdm(data_loader, desc=\"training...\"):\n",
    "        ids = batch[\"ids\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "def evaluate(data_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"evaluating...\"):\n",
    "            ids = batch[\"ids\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "            prediction = model(ids)\n",
    "            loss = criterion(prediction, label)\n",
    "            accuracy = get_accuracy(prediction, label)\n",
    "            epoch_losses.append(loss.item())\n",
    "            epoch_accs.append(accuracy.item())\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m metrics\u001b[38;5;241m=\u001b[39mcollections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m----> 9\u001b[0m     train_loss,train_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     val_loss,val_acc\u001b[38;5;241m=\u001b[39mevaluate(val_dataloader,device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     14\u001b[0m                                   model\u001b[38;5;241m=\u001b[39mmodel,criterion\u001b[38;5;241m=\u001b[39mcriterion\n\u001b[1;32m     15\u001b[0m                         )\n\u001b[1;32m     17\u001b[0m     metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[68], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data_loader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n\u001b[1;32m      8\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(ids)\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(prediction, label)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "### training the model\n",
    "save_dire=\"../saved_model/nbow_transformer.pt\"\n",
    "n_epochs=25\n",
    "best_valid_loss=float(\"inf\")\n",
    "\n",
    "metrics=collections.defaultdict(list)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss,train_acc=train(train_dataloader,device=device,\n",
    "                                  model=model,criterion=criterion,\n",
    "                                  optimizer=optimizer)\n",
    "    \n",
    "    val_loss,val_acc=evaluate(val_dataloader,device=device,\n",
    "                                  model=model,criterion=criterion\n",
    "                        )\n",
    "    \n",
    "    metrics['train_loss'].append(train_loss)\n",
    "    metrics['train_acc'].append(train_acc)\n",
    "    metrics['val_loss'].append(val_loss)\n",
    "    metrics['val_acc'].append(val_acc)\n",
    "    if val_loss<train_loss:\n",
    "       \n",
    "        best_valid_loss=val_loss\n",
    "        \n",
    "        torch.save(model.state_dict(),save_dire)\n",
    "\n",
    "    print(\"Epoch Num: \", epoch)\n",
    "    print('Train Loss: {:.3f}   | Train Acc: {:.3f}'.format(train_loss,train_acc))\n",
    "    print('Val Loss: {:.3f}   | Val Acc: {:.3f}'.format(val_loss,val_acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# best Validation loss\n",
    "\n",
    "model.load_state_dict(torch.load(save_dire))\n",
    "best_val_loss,best_val_acc=evaluate(val_dataloader,device=device,\n",
    "                                  model=model,criterion=criterion\n",
    "                                )\n",
    "\n",
    "print('Best Val Loss: {:.3f}   | Best Val Acc: {:.3f}'.format(best_val_loss,best_val_acc))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
