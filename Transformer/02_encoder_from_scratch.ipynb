{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "from torch import nn,Tensor\n",
    "from torch.nn import MultiheadAttention,LayerNorm,Embedding\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=('cuda:0' if torch.cuda.is_available() else 'cpu' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vanilla Transformer Encoder from scracth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Build Input Embdedding and add Positional Encoding to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a random tensor \n",
    "batch_size=8\n",
    "num_tokens=10\n",
    "\n",
    "example_tensors=torch.rand(size=(batch_size,num_tokens)).to(dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensors.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_tensor.shape: torch.Size([8, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "### Creating Embedding Layer \n",
    "embed_dim=256\n",
    "vocab_size=100\n",
    "emb_layer=nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_dim)\n",
    "\n",
    "##\n",
    "embedded_tensor=emb_layer(example_tensors)\n",
    "\n",
    "print(f'embedded_tensor.shape: {embedded_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define positional encoding function\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "    pe = torch.zeros(seq_len, embed_dim)\n",
    "    pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "    pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "    pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "    return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc=positional_encoding(seq_len=num_tokens,embed_dim=embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 256])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining Embedding Class now \n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_dim):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.emb_layer=nn.Embedding(num_embeddings=self.vocab_size,embedding_dim=self.embed_dim)\n",
    "\n",
    "    def positional_encoding(self,seq_len, embed_dim):\n",
    "        pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "        return pe\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.emb_layer(x)\n",
    "        seq_len=x.shape[1]\n",
    "        pos_enc=self.positional_encoding(seq_len=seq_len,embed_dim=self.embed_dim).to(x.device)\n",
    "        return torch.add(x,pos_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_layer=EmbeddingLayer(vocab_size=vocab_size,embed_dim=embed_dim).to(device)(example_tensors.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_layer[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Defining Transformer Encoder; \n",
    "\n",
    "1. First MHA\n",
    "2. Add and Norm Layer\n",
    "3. feedforward layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer=nn.Sequential(\n",
    "    nn.Linear(in_features=embed_dim,out_features=embed_dim*4),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(in_features=embed_dim*4,out_features=embed_dim)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFL(nn.Module): ## feedforward Layer\n",
    "    def __init__(self,embed_dim,dff,dropout):\n",
    "        super(FFL,self).__init__()\n",
    "        self.ffl_layer=nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim,out_features=dff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=dff,out_features=embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.ffl_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FeedForward Layer\n",
    "FFL(embed_dim=embed_dim,dff=embed_dim*4,dropout=0.3).to(device)(embedded_layer).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]],\n",
       "\n",
       "        [[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]],\n",
       "\n",
       "        [[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]],\n",
       "\n",
       "        [[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]],\n",
       "\n",
       "        [[-0.9979, -0.2928,  0.2799,  ..., -1.2851, -0.4490,  1.7955],\n",
       "         [-0.2755, -0.7590,  0.8744,  ..., -1.3427, -0.4938,  1.7849],\n",
       "         [-0.2078, -1.6316,  1.1672,  ..., -1.3345, -0.4877,  1.7848],\n",
       "         ...,\n",
       "         [-0.3582, -0.4737, -0.4577,  ..., -1.2477, -0.4053,  1.8535],\n",
       "         [-0.0468, -1.2494,  0.0753,  ..., -1.2029, -0.3866,  1.8016],\n",
       "         [-0.5243, -1.8473,  0.7113,  ..., -1.1474, -0.3587,  1.7551]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we will define base transformer Encoder (single layer); In NLP we usuallu apply layer normalization to emb_dimension only \n",
    "layer_norm=nn.LayerNorm(normalized_shape=embed_dim).to(device)\n",
    "\n",
    "\n",
    "layer_norm(embedded_layer+embedded_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,vocab_size,droput,num_heads):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        ##initializing variables\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.dff=self.embed_dim*4\n",
    "        self.num_heads=num_heads\n",
    "        self.dropout=droput\n",
    "        \n",
    "        ##nn.NN\n",
    "        self.mha=nn.MultiheadAttention(embed_dim=self.embed_dim,num_heads=self.num_heads,dropout=self.dropout)\n",
    "        self.layer_nomr0=nn.LayerNorm(normalized_shape=self.embed_dim)\n",
    "        self.layer_nomr1=nn.LayerNorm(normalized_shape=self.embed_dim)\n",
    "        self.layer_nomr2=nn.LayerNorm(normalized_shape=self.embed_dim)\n",
    "\n",
    "        ##Class Objects\n",
    "        \n",
    "        self.feedforward_layer=FFL(embed_dim=self.embed_dim,dff=self.dff,dropout=self.dropout)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        #x is of shape batch_size,seq_len,embed_dim; output from Embedding Layer\n",
    "\n",
    "        x=self.layer_nomr0(x)\n",
    "        mha_output,_=self.mha(x,x,x)\n",
    "        \n",
    "        normlized_1=self.layer_nomr1(x+mha_output)\n",
    "        ffd_output=self.feedforward_layer(normlized_1)\n",
    "        normlized_2=self.layer_nomr2(normlized_1+ffd_output)\n",
    "\n",
    "        return normlized_2\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output_shape: torch.Size([8, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "### checking TransformerEncoder\n",
    "## defining Encoder Parameter using baseline from Attention is all we need Paper\n",
    "num_heads=8\n",
    "num_layers=6\n",
    "\n",
    "encoder=TransformerEncoder(embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=0.3).to(device)\n",
    "\n",
    "print(f'encoder_output_shape: {encoder(embedded_layer).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers=[TransformerEncoder(embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=0.3)\n",
    "                           for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining Encoder with num_of_layers\n",
    "class TransformerEncoderLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_layers,embed_dim,vocab_size,droput,num_heads):\n",
    "        super(TransformerEncoderLayers,self).__init__()\n",
    "        self.num_layers=num_layers\n",
    "        self.embedding_layers=EmbeddingLayer(vocab_size=vocab_size,embed_dim=embed_dim)\n",
    "        self.transformer_encoders=nn.ModuleList([TransformerEncoder(embed_dim,vocab_size,droput,num_heads)\n",
    "                                  for i in range(self.num_layers)])\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.embedding_layers(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x=self.transformer_encoders[i](x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensors.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output_shape: torch.Size([8, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "### checking TransformerEncoder\n",
    "## defining Encoder Parameter using baseline from Attention is all we need Paper\n",
    "num_heads=8\n",
    "num_layers=6\n",
    "\n",
    "encoder_layers=TransformerEncoderLayers(num_layers=num_layers,embed_dim=embed_dim,vocab_size=vocab_size,\n",
    "                           num_heads=num_heads,droput=0.3)\n",
    "\n",
    "print(f'encoder_output_shape: {encoder_layers(example_tensors).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor=torch.LongTensor(size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0254,  1.2196, -0.9225,  ...,  1.4687,  0.4730, -0.2067],\n",
       "         [-0.3676,  0.7876, -1.0055,  ...,  1.3824,  0.9694,  0.1047],\n",
       "         [-0.6959,  0.2638, -0.5278,  ...,  1.3144,  1.5179,  0.2961],\n",
       "         ...,\n",
       "         [-0.1148,  0.0841, -0.5788,  ...,  1.3095,  0.7129,  0.6355],\n",
       "         [ 0.3884,  0.0905, -0.5934,  ...,  1.6724,  0.4144,  0.5311],\n",
       "         [-0.1556, -1.0732, -0.8616,  ...,  0.9064,  0.3509,  0.0690]],\n",
       "\n",
       "        [[-0.5176,  0.6613, -1.1251,  ...,  1.3231,  0.9872,  0.1495],\n",
       "         [-0.6503,  0.7359, -1.0623,  ...,  1.1830,  0.3046, -0.4694],\n",
       "         [-0.4996,  0.4626, -0.6042,  ...,  1.8565,  1.1727,  0.6076],\n",
       "         ...,\n",
       "         [-0.4223, -0.0653, -0.8974,  ...,  1.9943,  0.3851,  0.4359],\n",
       "         [-0.4256,  0.0300,  0.0233,  ...,  1.4695,  0.6478,  0.5803],\n",
       "         [-0.3249, -0.0456, -0.6723,  ...,  1.5817,  0.4523,  0.0068]],\n",
       "\n",
       "        [[-0.2590,  0.9130, -0.7669,  ...,  2.1335,  0.4574,  0.4472],\n",
       "         [-0.3560,  0.0356, -0.4256,  ...,  1.5115,  0.7877,  0.0286],\n",
       "         [-0.3298, -0.5289, -0.3918,  ...,  1.4243,  1.2070,  0.1845],\n",
       "         ...,\n",
       "         [ 0.3019,  0.5196, -1.1668,  ...,  1.3611,  0.4640,  0.5354],\n",
       "         [ 0.1396, -0.1959, -0.1087,  ...,  1.7678,  0.7073,  0.7344],\n",
       "         [-0.5767, -0.2505, -0.7551,  ...,  1.5553,  0.7586,  0.3886]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.2316,  0.9259, -1.1036,  ...,  2.0656,  1.0378, -0.3486],\n",
       "         [-0.6605,  0.1320, -0.8978,  ...,  1.7241,  0.8058,  0.0558],\n",
       "         [-0.5230, -0.4197, -0.8408,  ...,  1.2087,  0.5416,  0.3919],\n",
       "         ...,\n",
       "         [ 0.2353,  0.3674, -1.0329,  ...,  1.5430,  0.9455,  0.6910],\n",
       "         [-0.2665, -0.0747, -0.3025,  ...,  1.5947,  0.7075,  0.6533],\n",
       "         [-0.0241,  0.2639, -0.8127,  ...,  1.1099, -0.0396, -0.0621]],\n",
       "\n",
       "        [[-0.6191,  0.5449, -1.0585,  ...,  1.4470,  0.6678,  0.5098],\n",
       "         [-0.6560,  1.0408, -1.0041,  ...,  1.5548,  0.5473, -0.3520],\n",
       "         [-0.2187, -0.2815, -0.9264,  ...,  1.8765,  0.7610,  0.1019],\n",
       "         ...,\n",
       "         [ 0.4976,  0.1863, -0.4421,  ...,  1.4373,  0.7033,  0.6824],\n",
       "         [ 0.4479,  0.1128, -0.5141,  ...,  1.0350,  0.7718,  0.5799],\n",
       "         [-0.2893, -0.6148, -0.7287,  ...,  1.2970, -0.1415,  0.5186]],\n",
       "\n",
       "        [[-0.7928,  0.4913, -1.8379,  ...,  1.7702,  0.5333,  0.5196],\n",
       "         [-0.5241,  0.4646, -1.0375,  ...,  1.5000, -0.2511,  0.3544],\n",
       "         [-0.7372, -0.5932, -0.4254,  ...,  1.7367,  1.5805, -0.1593],\n",
       "         ...,\n",
       "         [ 0.0563,  0.5190, -0.2609,  ...,  1.3311,  1.2572,  1.0663],\n",
       "         [-0.1243,  0.0717, -0.2505,  ...,  2.1331,  1.0395,  0.5409],\n",
       "         [-0.2815, -0.3431, -0.6818,  ...,  0.9853,  1.0928,  0.3332]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### when both input_tensor and encoder_layers are in cpu \n",
    "\n",
    "encoder_layers(example_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "encoder_layers=encoder_layers.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0581,  1.0266, -1.3102,  ...,  1.9474,  0.4914, -0.0629],\n",
       "         [-0.4414,  0.3293, -0.6951,  ...,  1.4459,  0.5976, -0.3158],\n",
       "         [-0.2268, -0.7159, -0.0768,  ...,  1.8520,  1.2939,  0.3036],\n",
       "         ...,\n",
       "         [ 0.2306, -0.4386, -0.2850,  ...,  1.7340,  0.8953,  0.7186],\n",
       "         [ 0.8397, -0.3872, -0.3890,  ...,  1.8776,  0.7422,  0.8469],\n",
       "         [ 0.1667, -0.8590, -1.1839,  ...,  1.7494,  0.2738,  0.0736]],\n",
       "\n",
       "        [[-0.2761,  0.7627, -1.2999,  ...,  1.7479,  0.5531, -0.1315],\n",
       "         [-0.2227,  0.5590, -0.8152,  ...,  2.0304,  0.6778,  0.2967],\n",
       "         [-0.2613,  0.8290, -0.2587,  ...,  1.3891,  0.7312, -0.2080],\n",
       "         ...,\n",
       "         [ 0.3941, -0.1155, -0.3587,  ...,  1.6393,  1.2288,  1.3723],\n",
       "         [-0.1064, -0.6976, -1.2014,  ...,  1.7687,  1.1368,  1.0333],\n",
       "         [-0.5414, -0.3658, -0.6667,  ...,  1.7077,  0.2410,  0.4198]],\n",
       "\n",
       "        [[-0.5980, -0.1019, -0.9714,  ...,  1.7979,  0.8000,  0.0844],\n",
       "         [-0.3700, -0.0319, -0.8894,  ...,  1.2156,  1.0571,  0.1312],\n",
       "         [-0.4362,  0.6478, -0.8545,  ...,  1.4228,  0.8814,  0.1396],\n",
       "         ...,\n",
       "         [ 0.5920,  0.2340, -0.4783,  ...,  1.8030,  0.2143,  0.2656],\n",
       "         [ 0.0499, -0.2753, -0.2279,  ...,  1.9013,  1.0648,  0.8190],\n",
       "         [-0.5057, -0.8513, -0.9036,  ...,  1.4688,  0.5485,  0.2742]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.7196,  0.5405, -1.4309,  ...,  1.7239,  0.3920,  0.2535],\n",
       "         [ 0.3339, -0.0396, -1.0850,  ...,  1.4690,  0.8348, -0.5146],\n",
       "         [-0.3433,  0.0670, -0.9734,  ...,  1.5223,  0.8702,  0.0580],\n",
       "         ...,\n",
       "         [ 0.3046,  0.5521, -0.5417,  ...,  1.6065,  0.9716,  1.0762],\n",
       "         [ 0.5179, -0.2170, -0.9598,  ...,  1.8749,  0.4444,  0.3942],\n",
       "         [-0.1936, -0.6132, -0.8549,  ...,  2.0928,  0.9634,  0.1453]],\n",
       "\n",
       "        [[-0.9320,  1.1174, -1.5263,  ...,  1.2685,  0.6978, -0.3085],\n",
       "         [-0.1658,  0.3692, -0.3598,  ...,  1.3299,  0.6099, -0.0057],\n",
       "         [-0.8998, -0.5613, -1.2802,  ...,  1.4404,  0.9863,  0.1748],\n",
       "         ...,\n",
       "         [ 0.0981, -0.0173, -0.3569,  ...,  1.5040,  0.8232,  0.3571],\n",
       "         [ 0.8358,  0.0923, -0.1010,  ...,  1.9188,  0.9469,  0.5258],\n",
       "         [-0.8816, -0.7446, -1.0294,  ...,  1.2424,  0.8792,  0.2114]],\n",
       "\n",
       "        [[-0.4740,  1.1329, -0.9157,  ...,  1.6143,  0.4324,  0.0807],\n",
       "         [-0.1152,  0.4368, -0.8879,  ...,  1.6918,  0.9645,  0.2519],\n",
       "         [-0.3429, -0.2195, -0.9776,  ...,  1.9639,  0.7897,  0.3338],\n",
       "         ...,\n",
       "         [ 0.4917,  0.6812, -1.1083,  ...,  1.3867,  0.4458,  0.7727],\n",
       "         [-0.0403, -0.9212, -0.4970,  ...,  1.7769,  0.1597,  0.4873],\n",
       "         [ 0.0789, -0.7868, -0.6214,  ...,  1.4172,  0.8121,  0.1542]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layers(example_tensors.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor=torch.LongTensor(size=(10,10)).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tensor.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers=encoder_layers.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Both input and model are in cuda now \n",
    "\n",
    "### \n",
    "encoder_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
